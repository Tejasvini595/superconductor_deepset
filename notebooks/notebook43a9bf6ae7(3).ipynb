{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13606696,"sourceType":"datasetVersion","datasetId":8646411}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mendeleev shap mlflow \"numpy==1.26.4\" \"protobuf==5.29.5\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport mlflow\nimport mlflow.tensorflow\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport re\nimport time\nfrom mendeleev import element\nimport mendeleev\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport joblib\nimport os\n\n\nprint(\"LOADING AND PREPARING DATA\")\n# Load the new dataset\ndf = pd.read_csv(\"/kaggle/input/dataset1/superconductors_kaggle_ready_v2.csv\", sep=None, engine='python')\n\nprint(df.head())\n\n# Rename columns to match the expected format\ndf = df.rename(columns={\n    \"element\": \"composition\",\n    \"critical_temp_K\": \"Tc\"\n})\n\n# Clean the target variable\ndf['Tc'] = pd.to_numeric(df['Tc'], errors='coerce')\ndf = df[df['Tc'].notna()]\ny = df['Tc'].values\n\nprint(f\"Final dataset shape after cleaning: {df.shape}\")\n\n\nprint(\"PARSING CHEMICAL COMPOSITIONS\")\n\n# Extract chemical composition information\ndef parse_composition(composition_str):\n    \"\"\"Parse composition string to extract elements and their stoichiometry\"\"\"\n    if pd.isna(composition_str):\n        return {}\n\n    # Finding all element number pairs\n    pattern = r'([A-Z][a-z]?)(\\d*\\.?\\d*)'\n    matches = re.findall(pattern, composition_str)\n\n    element_dict = {}\n    for element, count in matches:\n        if count == '':\n            count = 1\n        else:\n            count = float(count)\n        element_dict[element] = count\n\n    return element_dict\n\n\nprint(\"EXTRACTING MENDELEEV FEATURES (22 FEATURES)\")\n\n# Get periodic table features (22 features as per paper)\ndef get_mendeleev_features(max_atomic_number=96):\n    \"\"\"Extract the 22 features mentioned in the paper using current mendeleev API\"\"\"\n    element_features = {}\n\n    feature_names = [\n        'atomic_number', 'atomic_volume', 'block_encoded', 'density', 'dipole_polarizability',\n        'electron_affinity', 'evaporation_heat', 'fusion_heat', 'group_id', 'lattice_constant',\n        'lattice_structure', 'melting_point', 'period', 'specific_heat', 'thermal_conductivity',\n        'vdw_radius', 'covalent_radius_pyykko', 'en_pauling', 'atomic_weight', 'atomic_radius_rahm',\n        'first_ionization_energy', 'valence_electrons'\n    ]\n\n    for atomic_num in range(1, max_atomic_number + 1):\n        try:\n            elem = element(atomic_num)\n            symbol = elem.symbol\n\n            features = [\n                elem.atomic_number,\n                elem.atomic_volume if elem.atomic_volume is not None else 0,\n                1 if elem.block == 's' else 2 if elem.block == 'p' else 3 if elem.block == 'd' else 4,\n                elem.density if elem.density is not None else 0,\n                elem.dipole_polarizability if elem.dipole_polarizability is not None else 0,\n                elem.electron_affinity if elem.electron_affinity is not None else 0,\n                elem.evaporation_heat if elem.evaporation_heat is not None else 0,\n                elem.fusion_heat if elem.fusion_heat is not None else 0,\n                elem.group_id if elem.group_id is not None else 0,\n                elem.lattice_constant if elem.lattice_constant is not None else 0,\n                1,\n                elem.melting_point if elem.melting_point is not None else 0,\n                elem.period,\n                elem.specific_heat if hasattr(elem, 'specific_heat') and elem.specific_heat is not None else 0,\n                elem.thermal_conductivity if elem.thermal_conductivity is not None else 0,\n                elem.vdw_radius if elem.vdw_radius is not None else 0,\n                elem.covalent_radius_pyykko if elem.covalent_radius_pyykko is not None else 0,\n                elem.en_pauling if elem.en_pauling is not None else 0,\n                elem.atomic_weight,\n                elem.atomic_radius_rahm if elem.atomic_radius_rahm is not None else 0,\n                elem.ionenergies[1] if len(elem.ionenergies) > 1 else 0,\n                elem.nvalence() if hasattr(elem, 'nvalence') else 0\n            ]\n\n            # Handle missing values\n            element_features[symbol] = [0.0 if pd.isna(f) or f is None else float(f) for f in features]\n\n        except Exception as e:\n            print(f\"Error processing element {atomic_num}: {e}\")\n            continue\n\n    print(f\"Successfully processed {len(element_features)} elements\")\n\n    return element_features\n\n# Get element features\nelement_features_dict = get_mendeleev_features()\n\n\nprint(\"CREATING DEEPSET INPUTS (23D VECTORS)\")\n\ndef create_deepset_input(composition_str, element_features_dict, max_elements=20, debug=False):\n    \"\"\"\n    Create input for DeepSet: each element becomes a 23-dimensional vector\n    (22 elemental features + 1 stoichiometry)\n    \"\"\"\n    element_composition = parse_composition(composition_str)\n\n    if not element_composition:\n        return np.zeros((max_elements, 23))\n\n    element_vectors = []\n\n    for element, stoichiometry in element_composition.items():\n        if element in element_features_dict and len(element_vectors) < max_elements:\n            elem_features = element_features_dict[element].copy()\n            elem_features.append(float(stoichiometry))\n            element_vectors.append(elem_features)\n\n    original_length = len(element_vectors)\n    while len(element_vectors) < max_elements:\n        element_vectors.append([0.0] * 23)\n\n    return np.array(element_vectors[:max_elements])\n\n\nprint(\"Creating DeepSet inputs...\")\nmax_elements = 10\nX_deepset_inputs = []\n\nprint(f\"\\nDetailed DeepSet creation for first 3 compositions:\")\nfor idx in range(min(3, len(df))):\n    composition = df.iloc[idx]['composition']\n    deepset_input = create_deepset_input(composition, element_features_dict, max_elements, debug=True)\n    X_deepset_inputs.append(deepset_input)\n\nprint(f\"\\nProcessing remaining compositions...\")\nfor idx in range(3, len(df)):\n    composition = df.iloc[idx]['composition']\n    deepset_input = create_deepset_input(composition, element_features_dict, max_elements)\n    X_deepset_inputs.append(deepset_input)\n\n    if idx % 1000 == 0:\n        print(f\"Processed {idx}/{len(df)} compositions\")\n\nX_deepset = np.array(X_deepset_inputs)\nprint(f\"DeepSet input shape: {X_deepset.shape}\")\n\n\nprint(\"DEFINING DEEPSET MODEL\")\n\nclass DeepSetSuperconductor(tf.keras.Model):\n    def __init__(self, latent_dim=300, **kwargs):\n        super(DeepSetSuperconductor, self).__init__(**kwargs)\n        self.latent_dim = latent_dim\n\n        # === φ network (per-element feature extractor, Conv2D-based) ===\n        self.phi_network = tf.keras.Sequential([\n            layers.Dense(992, activation='relu', name='phi_1'),\n            layers.Dense(768, activation='relu', name='phi_2'),\n            layers.Dense(512, activation='relu', name='phi_3'),\n            layers.Dense(384, activation='relu', name='phi_4'),\n            layers.Dense(256, activation='relu', name='phi_5'),\n            layers.Dense(128, activation='relu', name='phi_6'),\n            layers.Dense(latent_dim, activation='linear', name='phi_7'),\n        ], name='phi_network')\n\n        # === ρ network (aggregator over pooled latent representation) ===\n        self.rho_network = tf.keras.Sequential([\n            layers.Dense(960, activation='relu', name='rho_1'),\n            layers.Dense(832, activation='relu', name='rho_2'),\n            layers.Dense(768, activation='relu', name='rho_3'),\n            layers.Dense(640, activation='relu', name='rho_4'),\n            layers.Dense(512, activation='relu', name='rho_5'),\n            layers.Dense(384, activation='relu', name='rho_6'),\n            layers.Dense(256, activation='relu', name='rho_7'),\n            layers.Dense(192, activation='relu', name='rho_8'),\n            layers.Dense(160, activation='relu', name='rho_9'),\n            layers.Dense(128, activation='relu', name='rho_10'),\n            layers.Dense(96, activation='relu', name='rho_11'),\n            layers.Dense(64, activation='relu', name='rho_12'),\n            layers.Dense(1, activation='linear', name='rho_output'),\n        ], name='rho_network')\n\n    def call(self, inputs, training=None):\n        \"\"\"\n        Forward pass of DeepSet (Conv2D version)\n        inputs: (batch_size, max_elements, 23)\n        \"\"\"\n        batch_size = tf.shape(inputs)[0]\n        max_elements = tf.shape(inputs)[1]\n        feature_dim = tf.shape(inputs)[2]\n\n        # Flatten across elements to apply φ element-wise\n        reshaped_inputs = tf.reshape(inputs, [batch_size * max_elements, feature_dim])\n\n        # φ-network: Conv2D feature extraction per element\n        phi_outputs = self.phi_network(reshaped_inputs, training=training)\n\n        # Reshape back to (batch, max_elements, latent_dim)\n        phi_outputs = tf.reshape(phi_outputs, [batch_size, max_elements, self.latent_dim])\n\n        # Sum pooling across elements (permutation invariant)\n        pooled = tf.reduce_sum(phi_outputs, axis=1)\n\n        # ρ-network: fully convolutional aggregator → scalar output\n        output = self.rho_network(pooled, training=training)\n\n        return output\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({'latent_dim': self.latent_dim})\n        return config\n\n\n\nprint(\"STARTING 50 INDEPENDENT RUNS WITH MODEL STORAGE\")\n\n\n# Create directory for models\nos.makedirs('saved_models', exist_ok=True)\n\n# Storage for all runs\nall_test_predictions = {} \nall_metrics = []\n\nall_models = []\nall_scalers = []\n\nmlflow.set_experiment(\"deepset_superconductor_50_runs\")\n\nfor run_num in range(50):\n    \n    print(f\"RUN {run_num + 1}/50\")\n    \n\n    # Create fresh random split for this run with indices\n    indices = np.arange(len(X_deepset))\n    indices_temp, test_indices, X_temp, X_test, y_temp, y_test = train_test_split(\n        indices, X_deepset, y, test_size=0.20, random_state=run_num\n    )\n    indices_train, indices_val, X_train, X_val, y_train, y_val = train_test_split(\n        indices_temp, X_temp, y_temp, test_size=0.176, random_state=run_num\n    )\n\n    print(f\"Run {run_num + 1} - Data splits:\")\n    print(f\"  Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n    print(f\"  Test indices range: {test_indices.min()} to {test_indices.max()}, total: {len(test_indices)}\")\n\n    # FEATURE NORMALIZATION\n    X_reshaped = X_train.reshape(-1, 23)\n    scaler = StandardScaler()\n    X_train_normalized = scaler.fit_transform(X_reshaped).reshape(-1, max_elements, 23)\n\n    X_val_normalized = scaler.transform(X_val.reshape(-1, 23)).reshape(-1, max_elements, 23)\n    X_test_normalized = scaler.transform(X_test.reshape(-1, 23)).reshape(-1, max_elements, 23)\n\n    # Create and compile model\n    model = DeepSetSuperconductor(latent_dim=300)\n    model.build(input_shape=(None, max_elements, 23))\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae', 'mse']\n    )\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            patience=40,\n            restore_best_weights=True,\n            monitor='val_loss'\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            patience=15,\n            factor=0.5,\n            monitor='val_loss'\n        )\n    ]\n\n    # Training\n    with mlflow.start_run(run_name=f\"run_{run_num + 1}\") as mlflow_run:\n        mlflow.log_param(\"run_number\", run_num + 1)\n        mlflow.log_param(\"random_state\", run_num)\n\n        print(f\"Training run {run_num + 1}...\")\n        start_time = time.time()\n\n        history = model.fit(\n            X_train_normalized, y_train,\n            batch_size=64,\n            epochs=400,\n            validation_data=(X_val_normalized, y_val),\n            callbacks=callbacks,\n            verbose=1  # Show progress for each run\n        )\n\n        training_time = time.time() - start_time\n\n        # Make predictions\n        print(f\"  Making predictions on test set...\")\n        test_pred = model.predict(X_test_normalized, verbose=0)\n\n        # Store predictions for each test sample\n        print(f\"  Storing predictions for {len(test_indices)} test samples...\")\n        for idx, pred in zip(test_indices, test_pred.flatten()):\n            if idx not in all_test_predictions:\n                all_test_predictions[idx] = []\n            all_test_predictions[idx].append(pred)\n\n        # Calculate metrics\n        test_r2 = r2_score(y_test, test_pred)\n        test_mae = mean_absolute_error(y_test, test_pred)\n        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n\n        all_metrics.append({\n            'run': run_num + 1,\n            'test_r2': test_r2,\n            'test_mae': test_mae,\n            'test_rmse': test_rmse,\n            'training_time': training_time\n        })\n\n        mlflow.log_metric(\"test_r2\", test_r2)\n        mlflow.log_metric(\"test_mae\", test_mae)\n        mlflow.log_metric(\"test_rmse\", test_rmse)\n        mlflow.log_metric(\"training_time\", training_time)\n\n        # NEW: Save model to disk\n        model_path = f'saved_models/deepset_model_run_{run_num + 1}.h5'\n        scaler_path = f'saved_models/scaler_run_{run_num + 1}.pkl'\n        \n        model.save(model_path)\n        joblib.dump(scaler, scaler_path)\n        \n        print(f\"  Model saved to: {model_path}\")\n        print(f\"  Scaler saved to: {scaler_path}\")\n        \n        # NEW: Store model and scaler in memory for ensemble predictions\n        all_models.append(model)\n        all_scalers.append(scaler)\n\n        print(f\"Run {run_num + 1} completed in {training_time:.2f}s\")\n        print(f\"  Test R²: {test_r2:.4f}, MAE: {test_mae:.4f}K, RMSE: {test_rmse:.4f}K\")\n        print(f\"  Training epochs completed: {len(history.history['loss'])}\")\n        print(f\"  Final train loss: {history.history['loss'][-1]:.4f}, Final val loss: {history.history['val_loss'][-1]:.4f}\")\n        print(f\"  Total materials tracked so far: {len(all_test_predictions)}\")\n        print(f\"  Models stored in memory: {len(all_models)}\")\n        print()\n\n\n\nprint(\"AGGREGATING RESULTS FROM 50 RUNS\")\n\n\n# Calculate for materials tested at least 10 times\nmin_tests = 10\nqualified_materials = {idx: preds for idx, preds in all_test_predictions.items()\n                       if len(preds) >= min_tests}\n\nprint(f\"Materials tested at least {min_tests} times: {len(qualified_materials)}\")\n\n\npredicted_temps = []\nmeasured_temps = []\nerror_bars = []\n\nfor idx, predictions in qualified_materials.items():\n    predicted_temps.append(np.mean(predictions))\n    measured_temps.append(y[idx])\n    error_bars.append(np.std(predictions))\n\npredicted_temps = np.array(predicted_temps)\nmeasured_temps = np.array(measured_temps)\nerror_bars = np.array(error_bars)\n\n# Calculate overall metrics\noverall_rmse = np.sqrt(mean_squared_error(measured_temps, predicted_temps))\noverall_r2 = r2_score(measured_temps, predicted_temps)\n\nprint(f\"\\nOVERALL RESULTS (materials tested ≥{min_tests} times):\")\nprint(f\"  Number of materials: {len(qualified_materials)}\")\nprint(f\"  RMSE: {overall_rmse:.2f} K\")\nprint(f\"  R²: {overall_r2:.4f}\")\n\n# Metrics across all runs\nmetrics_df = pd.DataFrame(all_metrics)\n# print(f\"\\nMETRICS ACROSS 50 RUNS:\")\n# print(f\"  Mean Test R²: {metrics_df['test_r2'].mean():.4f} ± {metrics_df['test_r2'].std():.4f}\")\n# print(f\"  Mean Test MAE: {metrics_df['test_mae'].mean():.2f} ± {metrics_df['test_mae'].std():.2f} K\")\n# print(f\"  Mean Test RMSE: {metrics_df['test_rmse'].mean():.2f} ± {metrics_df['test_rmse'].std():.2f} K\")\n# print(f\"  Mean Training Time: {metrics_df['training_time'].mean():.2f} ± {metrics_df['training_time'].std():.2f} s\")\n\n# Plot results\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nax.errorbar(measured_temps, predicted_temps, yerr=error_bars,\n            fmt='o', alpha=0.6, capsize=3, label='SuperCon testset')\nax.plot([measured_temps.min(), measured_temps.max()],\n        [measured_temps.min(), measured_temps.max()],\n        'k--', alpha=0.5, label='Perfect prediction')\n\nax.set_xlabel('Measured Tc (K)', fontsize=14)\nax.set_ylabel('Predicted Tc (K)', fontsize=14)\nax.set_title(f'Predicted vs Measured Tc\\n50 runs, {len(qualified_materials)} materials tested ≥{min_tests} times\\nRMSE = {overall_rmse:.2f} K, R² = {overall_r2:.4f}', fontsize=16)\nax.legend(fontsize=12)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"SAVING ADDITIONAL RESULTS\")\n\n# Save element features dictionary (only once, needed for predictions)\njoblib.dump(element_features_dict, 'saved_models/element_features_dict.pkl')\nprint(\"Element features dictionary saved\")\n\n# Save all test predictions\nnp.save('saved_models/all_test_predictions.npy', all_test_predictions)\nprint(\"All test predictions saved\")\n\n# Save metrics\nmetrics_df.to_csv('saved_models/all_runs_metrics.csv', index=False)\nprint(\"Metrics saved\")\n\nprint(\"50 RUNS COMPLETED - ALL MODELS SAVED\")\nprint(f\"Total models in memory: {len(all_models)}\")\nprint(f\"Total scalers in memory: {len(all_scalers)}\")\n\n\n# NEW: DEFINE ENSEMBLE PREDICTION FUNCTION\nprint(\"DEFINING ENSEMBLE PREDICTION FUNCTION\")\n\ndef predict_tc_ensemble(composition, all_models, all_scalers, element_features_dict, \n                        max_elements=10, debug=False):\n    \"\"\"\n    Predict Tc using ensemble of all 50 models\n    \n    Args:\n        composition: Chemical formula string (e.g., \"YBa2Cu3O7\")\n        all_models: List of trained models\n        all_scalers: List of fitted scalers (one per model)\n        element_features_dict: Dictionary of element features\n        max_elements: Maximum number of elements to consider\n        debug: Print detailed information\n        \n    Returns:\n        mean_pred: Mean prediction across all models\n        std_pred: Standard deviation of predictions\n    \"\"\"\n    predictions = []\n    \n    if debug:\n        print(f\"Predicting Tc for: {composition}\")\n        print(f\"Using {len(all_models)} models in ensemble\")\n    \n    for i, (model, scaler) in enumerate(zip(all_models, all_scalers)):\n        # Create input\n        deepset_input = create_deepset_input(composition, element_features_dict, max_elements)\n        deepset_input = deepset_input.reshape(1, max_elements, 23)\n        \n        # Normalize using this model's scaler\n        normalized_input = scaler.transform(deepset_input.reshape(-1, 23)).reshape(1, max_elements, 23)\n        \n        # Predict\n        pred = model.predict(normalized_input, verbose=0)[0][0]\n        predictions.append(pred)\n        \n        if debug and i < 3:\n            print(f\"  Model {i+1} prediction: {pred:.2f}K\")\n    \n    mean_pred = np.mean(predictions)\n    std_pred = np.std(predictions)\n    min_pred = np.min(predictions)\n    max_pred = np.max(predictions)\n    \n    if debug:\n        print(f\"  ...\")\n        print(f\"\\nEnsemble Results:\")\n        print(f\"  Mean: {mean_pred:.2f}K\")\n        print(f\"  Std:  {std_pred:.2f}K\")\n        print(f\"  Range: [{min_pred:.2f}, {max_pred:.2f}]K\")\n        print(f\"  95% CI: [{mean_pred - 1.96*std_pred:.2f}, {mean_pred + 1.96*std_pred:.2f}]K\")\n    \n    return mean_pred, std_pred\n\n\nprint(f\"\\nEXAMPLE PREDICTIONS USING ENSEMBLE\")\ntest_compositions = [\"YBa2Cu3O7\", \"MgB2\", \"NbTi\", \"Pb\"]\nprint(f\"\\nNote: NbTi is not in the training dataset, actual Tc ≈ 10K\\n\")\n\nfor i, comp in enumerate(test_compositions):\n    try:\n        print(f\"\\nExample {i+1}: {comp}\")\n        mean_tc, std_tc = predict_tc_ensemble(comp, all_models, all_scalers, \n                                               element_features_dict, debug=True)\n        print(f\"\\nFINAL RESULT: {comp} -> Predicted Tc = {mean_tc:.2f} ± {std_tc:.2f}K\")\n        print(\"=\" * 60)\n    except Exception as e:\n        print(f\"Error predicting {comp}: {e}\")\n        print(\"=\" * 60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_compositions = [\n    \"La3Ni2O7\",\n    \"B4Mg\",\n    \"Mo0.94Tc1.06\",\n    \"C4Mo2.18Nb1.82\",\n    \"C120Rb8\",\n    \"C24KO2V\",\n    \"Ba0.6BiK0.4O2.3\",\n    \"B3Mg3N\",\n    \"C70CsK2\",\n    \"ClHfNNa0.125\",\n    \"Nb2.802SnZr0.198\",\n    \"Mo6S8Sr\",\n    \"Nb2.919SnV0.081\",\n    \"Nb3Sn0.81Ti0.19\",\n    \"C0.05N0.77Nb0.83Ti0.17\",\n    \"Ga1.04Nb6Sn0.96\",\n    \"Li0.88N3.48Nb3O0.52\",\n    \"Ga3Ru\",\n    \"Ag0.961Mo6S8\",\n    \"Nb2.61Sn0.9Ta0.39\",\n    \"B0.24O0.36V3Zr3\",\n    \"Mo3Pt\",\n    \"Ga0.1NNb0.9\",\n    \"N2NbTi\",\n    \"Bi2Ca\",\n    \"ClHfNNa0.288\",\n    \"HNb3Sn\",\n    \"B5Pd14Y2\",\n    \"La0.0039SiV2.9961\",\n    \"GaNb5Sn2\",\n    \"Ba0.65Bi0.75K0.31La0.29O3\",\n    \"Sb0.4Si1.6V6\",\n    \"Ga40.392Mo7.396\",\n    \"C6.156B1.764La5\",\n    \"B12Sc0.4Zr0.6\",\n    \"GaNi0.15V2.85\",\n    \"BrHfN\",\n    \"C59Na3.82\",\n    \"Si2V5.442Zr0.558\",\n    \"C16Br4P2Ta\",\n    \"C60Ba2.9K3.07\",\n    \"C60Rb\",\n    \"Mn3Mo18S24\",\n    \"N4Nb2.8Ti1.2\",\n    \"LiO2Ti\",\n    \"BBe2\",\n    \"BrNZr\",\n    \"Li0.64NbO2\",\n    \"Al2CaGe2\",\n    \"Al0.48B2.2Mg0.52\",\n    \"CoGa5U\",\n    \"GaSiV6\",\n    \"B4.026Mo2\",\n    \"B5Ir2.5Mo2.5\",\n    \"CB2Ni2Tb0.082Y0.918\",\n    \"Na0.11O3W\",\n    \"NbZr\",\n    \"Mo0.2Re0.8\",\n    \"Ga0.15Nb0.85\",\n    \"B6.6Ir0.7Mg2\",\n    \"BLuPd3\",\n    \"GaNb1.8V1.2\",\n    \"Al0.45B2Mg0.56\",\n    \"HfTaV4\",\n    \"Al5.598GeW1.402\",\n    \"Ba0.69Bi0.66Gd0.35K0.3O3\",\n    \"HfV4Zr\",\n    \"Au0.5Nb3Pt0.5\",\n    \"Hf0.5Nb0.2V2Zr0.3\",\n    \"Mo1.45Re0.55\",\n    \"N0.77Nb\",\n    \"B7Mo3Y\",\n    \"Hf0.4Nb1.2Zr0.4\",\n    \"Au0.3Nb3Pt0.7\",\n    \"Ag1.91Cs1.16Mo9S11\",\n    \"C3B4Ni4Y3\",\n    \"N0.86Nb2O0.14\",\n    \"Nb0.666Ti0.666Zr0.666\",\n    \"ClHfNNa0.25\",\n    \"Nb3Tl\",\n    \"LiMoS2\",\n    \"NbTi\",\n    \"CNb1.91\",\n    \"GaTi0.201V2.799\",\n    \"Bi2.9375Nb\",\n    \"C19Yb15\",\n    \"Si2Ta0.8V5.2\",\n    \"C19Er10Ru10\",\n    \"B2La3N2.91Ni2\",\n    \"GeSiV6\",\n    \"ErNb6Sn5\",\n    \"Mo6S8Sn0.854\",\n    \"C36RhZn9\",\n    \"TcW\",\n    \"Mo6OsRu\",\n    \"InLa3N\",\n    \"B32Rh27.2Ru4.8Y8\",\n    \"Mo0.81Ru0.19\",\n    \"HfTc\",\n    \"Tc0.85W0.15\",\n    \"HfNb\",\n    \"Au0.5Nb1.5\",\n    \"Mo1.3Rh0.7\",\n    \"La2.667Se4\",\n    \"INTi\",\n    \"P2Pb2\",\n    \"B2BaRh2\",\n    \"MoTc\",\n    \"Nb0.3Ti0.7\",\n    \"Mn0.51SiV2.49\",\n    \"Ga2Nb3\",\n    \"Hf3N4\",\n    \"BaSi6\",\n    \"BRh3SiY\",\n    \"B0.1V1.9Zr\",\n    \"Nb14S5\",\n    \"Sn0.85Zn0.15\",\n    \"Au0.3Ge0.7Nb3\",\n    \"CTl\",\n    \"B1.76Be0.24Zr\",\n    \"Co2Mo3\",\n    \"Ga0.375V0.625\",\n    \"Cd0.05Pb0.95\",\n    \"Ga40.88Mo8S0.86\",\n    \"Au0.938Cr0.062Nb3\",\n    \"Ir0.88Mo2.12\",\n    \"O0.67V3Zr3\",\n    \"Al0.45Mg1.55Si\",\n    \"N0.43Ta\",\n    \"In1.39Mo15Sc1.91Se19\",\n    \"Au0.3Nb0.7\",\n    \"B4Nb3\",\n    \"CoNb4P\",\n    \"Mo9Se11\",\n    \"Pb4.998S1.002\",\n    \"Mo0.09Nb2.91Sn\",\n    \"Sn0.6Tl0.4\",\n    \"B2SiV5\",\n    \"Bi3Se4\",\n    \"Si1.2Sn0.8V6\",\n    \"B2Ta0.5Zr0.5\",\n    \"GaNb6Pt\",\n    \"Si2Ti1.122V4.878\",\n    \"Pb5\",\n    \"P2Pt6Sr\",\n    \"Nb0.2Ti0.8\",\n    \"Pb2.4Tl1.6\",\n    \"N9Nb3.72Ta4.28\",\n    \"MgNi2\",\n    \"Cd3Mo18S24\",\n    \"Ir0.94Mo3.06\",\n    \"HfTa\",\n    \"Bi1.185K0.57O3.4\",\n    \"Pb0.64Sb0.36\",\n    \"CB2Ni2Tb0.328Y0.672\",\n    \"Nb14.5Re43.5\",\n    \"Au0.75Ir0.25Nb3\",\n    \"H0.07NbSe2\",\n    \"Mo18Se24Zn3\",\n    \"Nb6Sb1.6Sn0.4\",\n    \"Hf1.2Nb1.8Sn\",\n    \"BaBiSeSn\",\n    \"NbTa\",\n    \"Pb3.4Tl0.6\",\n    \"C4Th4\",\n    \"ReW\",\n    \"B4Y\",\n    \"RhSe\",\n    \"C2BBr2La3\",\n    \"HfTi\",\n    \"Mo1.2Si2V4.8\",\n    \"B2Mo0.72Nb0.28\",\n    \"In2.4Pb1.6\",\n    \"Mo0.3Tc1.7\",\n    \"Nb6RhRu\",\n    \"CAl0.1W0.9\",\n    \"B24Mo10U5\",\n    \"Pb1.2Sn2.8\",\n    \"C2B3Y2\",\n    \"C5Nb6\",\n    \"Rh3Se8\",\n    \"Hg3\",\n    \"B2MgNi2.5\",\n    \"BN0.975Nb2\",\n    \"I1.76Mo6S5.96\",\n    \"Ba3.04Bi3.37K0.96Na0.63O12\",\n    \"RuV\",\n    \"Cr1.2GaV1.8\",\n    \"Ge0.125Pb0.875\",\n    \"Nb3Sn2Ti3\",\n    \"B1.1Rh\",\n    \"B0.6N0.4Zr\",\n    \"Pt7Sc4Si2\",\n    \"Ru0.6W0.4\",\n    \"Nb0.33Ta0.33V0.34\",\n    \"Cs0.22O3W\",\n    \"Li0.795NbO2\",\n    \"Mo9Se11Tl2\",\n    \"Rh5Ti3\",\n    \"C70Cs1.09Na2.24\",\n    \"C2HoNi\",\n    \"ReTc\",\n    \"Nb6PdRh\",\n    \"SiSnV6\",\n    \"NbV\",\n    \"C2Ni0.96Th2\",\n    \"Nb0.33U0.33Zr0.34\",\n    \"B6Ta5\",\n    \"Sb0.32Sn3.68\",\n    \"B2Mo0.5Zr0.5\",\n    \"Ga0.51Sn0.49V3\",\n    \"N4Zr3\",\n    \"Re25Zr21\",\n    \"EuMo6S8\",\n    \"NbRhSi\",\n    \"N0.58Nb\",\n    \"PZr\",\n    \"Ga1.02Sb0.98V6\",\n    \"Re5.68V2.32\",\n    \"Au6Ba8Si40\",\n    \"Pb0.26Sn0.74\",\n    \"Pd0.08Sn0.92\",\n    \"MoNTa\",\n    \"Re22.8V7.2\",\n    \"C0.96Mo1.93\",\n    \"Ge3V5\",\n    \"Ga1.5IrNb5\",\n    \"NNbO\",\n    \"Mo0.8Ru1.2\",\n    \"Ge1.2Sn0.8V6\",\n    \"CY2\",\n    \"Ba7.96Ga10Si34.01\",\n    \"TaTi\",\n    \"Rh3S3\",\n    \"Lu2Ru3Si5\",\n    \"BePd\",\n    \"Ba7.87Ge11.97Si33.31\",\n    \"Re4Si2Th\",\n    \"Ba0.7BiK0.3O2.91\",\n    \"C2Ce0.5Lu0.5\",\n    \"FLuS\",\n    \"Ba2Si4\",\n    \"Ti0.05V0.95\",\n    \"NbU\",\n    \"Mo4N7\",\n    \"AuSb3\",\n    \"Mo1.95Os1.05\",\n    \"In1.74Sn0.26\",\n    \"SbTl\",\n    \"CClLa2\",\n    \"Ba4Bi0.4O12Pb3.6\",\n    \"AlBMo\",\n    \"Ir3Zr\",\n    \"C4.79Ni4.96Th3\",\n    \"MoRe2\",\n    \"Ge1.305Pt0.67Y\",\n    \"La9.6S16Y2.4\",\n    \"ClN2NaSZr2\",\n    \"BGe0.3Mo1.7\",\n    \"Co0.3GaV2.7\",\n    \"Ti0.89V0.11\",\n    \"CAu\",\n    \"C2Br3La3\",\n    \"C2Th\",\n    \"Ge22Mo11.75Rh1.25\",\n    \"TeTl2\",\n    \"Re0.975V0.025\",\n    \"B2Ta0.5Ti0.5\",\n    \"N0.81V\",\n    \"Bi4O7Rb2\",\n    \"Si3Yb\",\n    \"IrTa\",\n    \"Ge2W\",\n    \"B3Nb3Ru3\",\n    \"CB2ErRh2\",\n    \"B4Ru5Sc2\",\n    \"N2.62Nb4\",\n    \"BiKO2\",\n    \"RhScSi2\",\n    \"Ca0.08CuLa1.92O3.99\",\n    \"BaCu2O7SrTlY\",\n    \"Ba2Cu2.92O6Y\",\n    \"As2Ba0.52Fe2K0.48\",\n    \"Br2CuO2Sr2\",\n    \"Ba2Ca0.14Co0.12Cu2.88O6.96Y0.86\",\n    \"Ba1.9Ca0.2Cu3La0.1O7Y0.8\",\n    \"Ba4Cu6.91O14.94Y2\",\n    \"CuLa2O4.178\",\n    \"As2Ba0.5Fe2K0.4Sn0.1\",\n    \"Ba2Ca0.9Cu2.14O8Tl1.96\",\n    \"Ba2Cu2.89O6.66Y\",\n    \"Ba2Cu2.88LaO6.4\",\n    \"BaCa0.2Cu2.8O7.8P0.2SrY0.8\",\n    \"Ba11Bi9Cu4O29\",\n    \"CuNd2O3.864\",\n    \"As10Ca6Fe7.627Pt6.373\",\n    \"Cu12.0032Na21O50\",\n    \"BaCuFeO5Y\",\n    \"Ba4Cu6.72Nd2O15.08\",\n    \"As2Ba0.61Fe2K0.35Sn0.04\",\n    \"Ba4Cu6.74O14.6Y2\",\n    \"Ba2Bi0.22CuHg0.78O4.28\",\n    \"Ba1.93Cu3K0.07O7Y\",\n    \"Fe15.631K8.622Se20\",\n    \"Ba2Cu2.79Fe0.15O6.96Y\",\n    \"As3CaFe4\",\n    \"Ba2.668Ce1.332Cu5.88O17.8Sm4\",\n    \"As2Ba0.32Fe2K0.68\",\n    \"CrCu4O19.9Sr8Tl3\",\n    \"Ca0.5Cu2O6.3Pb0.46Sr2.46Y0.5\",\n    \"Ba2Cu3O6.072Y\",\n    \"Ba4Cu6.98O14.8Y2\",\n    \"Ba2Co0.03Cu2.97O6.94Y\",\n    \"Ba2Cu3.904Fe0.052O7.895Pr0.19Y0.79\",\n    \"As2Ba0.68Fe2K0.24Sn0.07\",\n    \"Ca0.12CuLa1.88O3.99\",\n    \"Bi3.86Ca0.87Cu2.74O14Sr3.62\",\n    \"Fe15.29K9.223Se20\",\n    \"Ca1.1Cu2La1.9O5.95\",\n    \"Ba2Cu3La0.7O6.97Pr0.3\",\n    \"C0.144Ba2Cu2.85O7.162Y\",\n    \"Ba1.95Cu3Nd1.05O6.95\",\n    \"Ba4Cu6.76O14.54Y2\",\n    \"Ba1.2Ca0.6Cu3La0.9O7.11Y0.3\",\n    \"Au0.17Ba2Ca2Cu3Hg0.69O8.3\",\n    \"Ag1.592Cu0.4Pd3Se4\",\n    \"As2Ba0.34Fe2K0.66\",\n    \"Ca0.1CuLa1.9O4.058\",\n    \"CuLa1.85O3.6Sr0.15\",\n    \"Cu2GdLa0.098O8RuSr1.902\",\n    \"Ba1.98Cu3O7Pr0.11Y0.91\",\n    \"CuLa1.94Na0.06O4\",\n    \"Cu2.97Mo18S24\",\n    \"Co0.015Cu0.985La1.8O4Sr0.2\",\n    \"As2Ca0.73FePr0.27\",\n    \"Ba4Cu6.72O14.63Y2\",\n    \"Ba3.87Ca0.246Cu6.73O14.63Y1.884\",\n    \"Bi0.45CaCu2.55O7.07Sr2\",\n    \"Al0.42Ba4Cu6.58Er2O14.88\",\n    \"Bi2.1Ca2Cu3O10Sr1.9\",\n    \"Ce0.3CuNd1.7O4\",\n    \"Ba2Cu2.5Fe0.5O7Y\",\n    \"CaCu2.5LaO6.952Pb0.5Sr\",\n    \"Ba1.6Ca0.1Cu3O6.104Sr0.4Yb0.9\",\n    \"Cu1.71O3Sr0.57Y0.43\",\n    \"Ba1.24Ca0.51Cu3La0.99Nd0.26O7.085\",\n    \"CuLa1.978O4Sr0.022\",\n    \"Ba4Cu6.84Ho2O14.36\",\n    \"Cu3O6.84Sr2Y\",\n    \"Bi0.33Ca0.89Cu2O7Sr2Tl0.78\",\n    \"Ca0.81Cu2O7Sr2Tl1.19\",\n    \"Co0.025Cu0.975La1.8O4Sr0.2\",\n    \"Ba1.98Cu2.91O6.47Y0.98\",\n    \"CCa1.11Cu2O7Sr1.89\",\n    \"Bi10Cu5O29Sr10\",\n    \"Ba1.32Ca0.344Cu3La1.13O7.18Y0.172\",\n    \"Ba2Ca0.424Cu2O7Pr0.576Tl\",\n    \"Al0.16Ba2Ca0.05Cu2.84O6.75Y0.95\",\n    \"BaBiCu\",\n    \"Ba4Cu6.83O14.6Y2\",\n    \"Ba2Cu2NbNdO7.86\",\n    \"Ba2Cu3O7.34Y\",\n    \"Ba1.4Cu3O6.94Sr0.6Yb\",\n    \"BaCu2.9ErO7.01P0.1Sr\",\n    \"Ba2Cu3NdO6.92\",\n    \"Cu1.98Fe1.02O7.36Sr2Y\",\n    \"Ba2Ce0.5Cu2Eu1.5O9Tl\",\n    \"Ce0.165CuNd1.835O3.8\",\n    \"Bi2CaCu2O9.07Sr2\",\n    \"Cu0.15O6Ru0.85Sr2Y\",\n    \"Cu1.5LaO5.008Pb0.5Sr\",\n    \"Ba1.5Cu3Nd1.5O6.96\",\n    \"Ba1.96Ca0.93Cu1.96O8.17Tl2\",\n    \"CuLa2O3.95\",\n    \"Ba2Ca0.29Cu3Er0.71O6.74\",\n    \"Ba2Cu3LaO6.7\",\n    \"Ba2Cu3O6.958Y\",\n    \"Ba2Ca0.18Co0.24Cu2.76O7Y0.82\",\n    \"Cu0.73LaMo0.25O3Zn0.03\",\n    \"Fe4Si9.9Y1.2\",\n    \"BaCuO5Tm2\",\n    \"Ba1.6Cu3O6.49Sm1.4\",\n    \"CCa2CuO5\",\n    \"Ba2Ca0.83Cu2O6.75Tl1.17\",\n    \"Ce0.05CuNd1.95O4\",\n    \"Ba2Cu3NdO6.52\",\n    \"Ba2Cu2.88O6.61Y\",\n    \"BaCa0.49Cu2.8La1.51O6.48Pt0.2\",\n    \"B8Fe3Nb7\",\n    \"Ba1.5Ca0.5Cu3LaO6.72\",\n    \"Cu2Gd2O9Pb0.5Sr2Tl0.5\",\n    \"Ce0.58Cu2Eu1.8O10.024Ru0.92Sr1.7\",\n    \"Cu2GdO7.96RuSr2\",\n    \"Ba2Cu3O6.96Yb\",\n    \"Ba1.2Ca0.6Cu2.977Dy0.3La0.9O7.004\",\n    \"Ba2Ca2Cu3.18Hg0.82O12.13\",\n    \"CuEu0.02O1.96Sr0.98\",\n    \"Ca0.1CuLa1.9O3.99\",\n    \"Ba1.04Ca0.78Cu2O7Sr0.96Tl1.22\",\n    \"Ba3.888Ca0.17Cu6.73O14.4Y1.942\",\n    \"Bi0.3Ca1.74Cu3Hg0.7O8.91Sr2.26\",\n    \"Ba2Ca0.93Cu2K1.12O8.46Tl0.88\",\n    \"Fe0.925Se\",\n    \"Ba2Cu2Hg1.75O8.02Pb0.25Y\",\n    \"Bi2.08CuO6Sr1.84\",\n    \"CuNd1.2O3.765Sr0.4Y0.4\",\n    \"Br2Ca2CuO2\",\n    \"Ca0.08CuLa1.92O4.07\",\n    \"Ba1.75Cu3La0.25O7Y\",\n    \"Ba1.82Cu3K0.18O7Y\",\n    \"Ba2CuHgO4.069\",\n    \"Ca1.85Cu3.22O10Sr2Tl1.93\",\n    \"Ba2Ca2Cu3Hg0.692O8.6\",\n    \"Cu1.83Mo3S4\",\n    \"Ba2Ce0.15CuHg0.85O4.15\",\n    \"Ca0.2Cr0.15Cu2.85O8Pb1.75Sr2Y\",\n    \"BaCu3O6.48SrY\",\n    \"Ca0.12CuLa1.88O3.914\",\n    \"Ba4Cu6.92O14.33Y2\",\n    \"As2Ba0.78Fe2K0.12Sn0.1\",\n    \"Ca0.27Cu2.969O7.73Pb2Sr1.968Y0.73\",\n    \"Ba2Cu2.93O7Y\",\n    \"Ba2Ca1.856Cu3.276O10Tl1.864\",\n    \"Ba2Cu2.84O6.58Y\",\n    \"CuO5Pr0.6Sr1.6Tl0.8\",\n    \"Cu2GdO7.94RuSr2\",\n    \"Ba8Ca8Cu12O39Tl7\",\n    \"Ba2CuHgO4.054\",\n    \"Ba2Cu2.94LaO7\",\n    \"Co0.1Cu0.9La1.8O4Sr0.2\",\n    \"Ba1.33Ca0.18Cu3La1.14O7.32Pr0.14\",\n    \"Al0.1Ba2Cu2.89O6.14Y\",\n    \"Ba1.9Cu3EuO6.93Pr0.1\",\n    \"CuGd0.89La0.9O4Sr0.21\",\n    \"Al0.11Ba2Cu2.89HoO6.53\",\n    \"Ba0.72Ca1.84Cu3O9Sr1.28Tl1.16\",\n    \"Ca0.73Cu2O4Sr1.19\",\n    \"Ba1.936Cu3O7Pr0.532Y0.532\",\n    \"Ba4Cu3DyO9.09\",\n    \"Ba1.6Cu3O6.999Sr0.4Yb\",\n    \"Ca0.12CuLa1.88O4.058\",\n    \"Ba1.8Cu3La0.2O7.05Y0.94\",\n    \"Ba2Co0.391Cu2.609O7.23Y\",\n    \"Ba2Cu2.98O6.96Y\",\n    \"Ca1.04Cu2.5Hg0.7O7.5Sr1.96Tl0.8\",\n    \"Bi0.5Ca0.4Cu2Hg0.5Nd0.6O6.6Sr2\",\n    \"Al0.11Ba2Cu2.725O6.4Tb0.182Y0.8\",\n    \"Ca0.91Cu2O7Pb0.5Sr2Tl0.59\",\n    \"BaFe1.95Se3\",\n    \"Al0.06Ba2Ca0.06Cu2.94O6.81Y0.94\",\n    \"Ba1.7Cu3Nd1.3O6.8\",\n    \"Ba2Cu1.16Hg0.84O4.19\",\n    \"Ba2Cu2.55Fe0.45O7.2Y\",\n    \"Bi0.5CaCu1.5O4Sr\",\n    \"Bi1.8Ca1.2Cu2.2O8.22Sr1.8\",\n    \"Ba1.8Cu3EuO6.94Pr0.2\",\n    \"Ba2Ca0.1Cu3O7Pr0.1Y0.8\",\n    \"Ba1.5Ca2Cu3O9.784Sr0.5Tl1.81\",\n    \"Ba1.5Ca0.5Cu3LaO6.98\",\n    \"C0.5Cr0.15Cu2N0.5O10Sr4Tl0.85\",\n    \"Ba2Cu1.059O6Tl1.941\",\n    \"CCu1.85Fe0.3O10Sr4Tl0.85\",\n    \"Ba1.9Cu3Eu1.1O7\",\n    \"As6Ca2.56Fe7.49Na0.44Nb0.51\",\n    \"Ba1.92Ca1.9Cu2.91O10Tl2.27\",\n    \"CuLa1.908O3.66Sr0.092\",\n    \"Au0.16Ba4Cu6.84Er2O15\",\n    \"Ba2Cu2.64Fe0.36O6.41Y\",\n    \"Al0.58Ba1.47Ca0.53Cu2.42LaO6.68\",\n    \"Ba2Cu3O5.72Y\",\n    \"As2Ba0.9Fe2Sn0.1\",\n    \"Cu1.2Pd1.8Y\",\n    \"Ba2Ca0.93Cu2O7.86Tl1.81\",\n    \"Bi2.15Ca0.75Cu2O8Sr1.92\",\n    \"BaCu3NdO7.13Sr\",\n    \"Cu4.23Mo18S24\",\n    \"Ba2Ca0.07Cu3O6.85Y0.93\",\n    \"Ba2Cu2.85O7Re0.15Y\",\n    \"BaCa0.4Cu2.8O7.8P0.2SrY0.6\",\n    \"CuLa0.7O5SrTl1.3\",\n    \"Ba1.8Cu3O6.97Sr0.2Yb\",\n    \"Ba1.5Ca0.5Cu3LaO7.06\",\n    \"Ba1.5Ca0.5Cu3LaO6.64\",\n    \"Au0.099Ba2Cu2.901O4.5Y\",\n    \"Ba2Cu2O10Y4\",\n    \"CaCu2O3\",\n    \"Ba1.986Ca0.035Cu4O8Y0.979\",\n    \"Al0.1Ba2Cu2.874O6.31Y\",\n    \"Ce0.4Cu2Gd1.6O9Pb0.5Sr2Tl0.5\",\n    \"Ba2Cu2.9O7.049Re0.1Y\",\n    \"Ba2Cu3NdO6.57\",\n    \"BaCuO6SrTl2\",\n    \"CuO4Sm1.9Sr0.1\",\n    \"Ba2Ca0.09Cu3Er0.91O6.92\",\n    \"BaCa0.25Cu2.8O7.8P0.2SrY0.75\",\n    \"Au0.099Ba2Cu2.901O6.5Y\",\n    \"Ba2CuHg0.96O4.34\",\n    \"Ba2Ca2Cu3O8.84Tl0.93\",\n    \"Ca0.48Cu2O6.96Pr0.52Sr2Tl0.94\",\n    \"Bi2Cu2O8.5PrSr2\",\n    \"Ce0.7Cu2Gd1.3O9.74RuSr2\",\n    \"Ba2Cu2.85O6.94YZn0.15\",\n    \"CuO2.02Pr0.17Sr0.83\",\n    \"Ba1.6Cu3O6.201Sr0.4Yb\",\n    \"CuLaNdO4\",\n    \"CuLa1.85O3.96Sr0.15\",\n    \"Ba2Cu2.88LaO6.8\",\n    \"Cu2O7.76RuSr2Y\",\n    \"Ce0.7Cu2Gd1.3O9.8RuSr2\",\n    \"Ba1.5Ca0.5Cu3LaO6.32\",\n    \"Ba2Cu2.94Fe0.06O6.36Y\",\n    \"Ba4Cu6.78Nd2O14.85\",\n    \"Ba25.96Cu64Eu6.04P120\",\n    \"Ba1.936Cu3O7Pr0.432Y0.632\",\n    \"Al0.3Ba1.92Ca0.2Cu2.7O6.76Y0.88\",\n    \"Ce0.27Cr0.35Cu2.65O9Sr2Y1.73\",\n    \"Ca0.91CuO2Sr0.09\",\n    \"Ba2Cu2.88Fe0.12O6.26Y\",\n    \"Ba1.6Ca0.35Cu3O6.075Sr0.4Yb0.65\",\n    \"Cu3NbO8Ta\",\n    \"BiCu2O8PbSr2Y\",\n    \"CCr0.54Cu2Hg0.46O9.88Sr4\",\n    \"Cu2FeO7.12Sr2Y\",\n    \"Ba2Cu2.97O6.89Tm\",\n    \"Bi0.3Ca3Cu4Hg0.7O10.74Sr2\",\n    \"Ba2Ca0.15Co0.36Cu2.64O7.05Y0.85\",\n    \"Fe2ScSi2\",\n    \"Ba1.932Cu3O7Pr0.634Y0.434\",\n    \"Ba2Cu5F14\",\n    \"Bi2CaCu2O9Sr2\",\n    \"B0.6Cu2.4O6.13Sr2Y\",\n    \"Ba1.9Ca0.94Cu1.9O8Tl2.26\",\n    \"Cu1.999ErFe1.001O7.36Sr2\",\n    \"Ba2Cu2.5O7Pd0.5Y\",\n    \"Bi2Ca0.3Cu4O10Sr2.7\",\n    \"Au0.08Ba2Cu2.92O7.04Y\",\n    \"Ba2Ca0.4Cu2Hg0.73O6.6Y0.6\",\n    \"Ba4Cu7O14.3Y2\",\n    \"Cu2GdIrO8Sr2\",\n    \"Ba2Cu2.77Fe0.23O7.13Y\",\n    \"Ba1.94Cu3O7Sm\",\n    \"Ba2Cu2.79Fe0.15O6.92Y\",\n    \"Ca6.925Cu23.15Nd3.66O41Sr2.925Y1.34\",\n    \"Ba1.73Cu3Nd1.27O6.86\",\n    \"Ba4Cu6O13Y2\",\n    \"Ba2Cu3ErO6.45\",\n    \"Cu2.601Mo9S12\",\n    \"C0.35Ba2Cu2.95O7.65Y\",\n    \"Bi2.09CaCu2O8.22Sr1.9\",\n    \"Ba2Ca1.93Cu2.862O9Tl1.07\",\n    \"Ba4Cu3GdO9.18\",\n    \"Fe0.056SbTi2.944\",\n    \"Ba4Cu6.77O14.26Y2\",\n    \"Ba2Co0.17Cu2.83O6.54Y\",\n    \"Ba2Cu3NaO6\",\n    \"Ba1.5Ca0.5Cu3LaO6.18\",\n    \"Bi1.916CuO5.482Sr1.84\",\n    \"Ba2Ca0.72Cu2O8Tl2.16\",\n    \"Ba2Cu3O6.967Y\",\n    \"Ba2CaCu2.13Hg0.87O6.64\",\n    \"Ba1.12Ca0.57Cu3La0.83O7.07Pr0.27\",\n    \"Cu2.04Gd0.88O8.02Ru0.96Sr2.12\",\n    \"Ce0.5Cu2Eu1.5Hg0.75O9Sr2W0.25\",\n    \"AuBa2Ca0.3Cu2O7Y0.7\",\n    \"CuLa1.58O4Pb0.27Sr0.15\",\n    \"Ba2Cu3LuO6\",\n    \"BaCuLu2O5\",\n    \"Ba2Cu3ErO6.98\",\n    \"CBi1.5Cu2O11Pb0.5Sr3.5\",\n    \"Ba1.6Ca0.2Cu3O6.166Sr0.4Yb0.8\",\n    \"Ba2Cu3O6.877Y\",\n    \"Ba1.968Cu3O7Pr0.316Y0.716\",\n    \"Ba1.95Cu3Na0.05O7Y\",\n    \"Ba4Ca5Cu7Hg1.44O20Re0.5\",\n    \"Ba2Ca1.9Cu3O10.94Tl1.82\",\n    \"Ba2Cu2.91O6.58Y\",\n    \"Ba0.15CaCu2La1.85O6\",\n    \"Fe4Sc1.2Si9.86\",\n    \"Ba2Ca0.84Cu2O8Tl1.94\",\n    \"CuLa1.76O3.92Sr0.24\",\n    \"Cu2.45Gd1.74Mo0.55O8Sr1.26\",\n    \"CuLa2O4.024\",\n    \"Ba2Cu3ErO6.99\",\n    \"Ba4Cu6.87Er2O15\",\n    \"Ba2Ca0.2Cu2Hg1.4O8Tl0.6Y0.8\",\n    \"Ba2Ca0.27Co0.36Cu2.64O7.05Y0.73\",\n    \"CaFe2P2\",\n    \"Ba1.8Cu3Eu1.2O6.67\",\n    \"Ba4Cu6.9Er2O14.3\",\n    \"Fe1.13S0.05Te0.95\",\n    \"Ba1.71Cu3K0.29O7Y\",\n    \"BaCu3O7.03SrY\",\n    \"Ba2Cu2.941O6.822Y\",\n    \"Ca0.12CuLa1.88O4.05\",\n    \"Ca0.77Cu2Hg0.3Nd0.23O7Pb0.7Sr2\",\n    \"Cu2O5S2Sc2Sr3\",\n    \"Ba1.5Ca0.5Cu3LaO6.9\",\n    \"Ba2Ca1.07Cu2O8Tl1.93\",\n    \"CuF1.08La0.813O1.92Sr0.187\",\n    \"Cu2.1Mo6S4.5Se3.5\",\n    \"Ba1.96Cu3K0.04O7Y\",\n    \"Ba2Ca0.15Cu3O6Y0.85\",\n    \"Ba1.988Ca0.043Cu4O8Y0.969\",\n    \"Cu2ErO7.84RuSr2\",\n    \"Ba2Cu4O8.06Y\",\n    \"Cu1.999EuFe1.001O7.36Sr2\",\n    \"CaCu2O7Pb0.7Sr1.82Tl0.3\",\n    \"Ba1.4Ca0.172Cu3La1.31O7.16Y0.086\",\n    \"Ba2CuHgO4.172\",\n    \"Ba1.6Ca0.25Cu3O6.072Sr0.4Yb0.75\",\n    \"Ba2CaCu2Hg0.7O7.8Tl1.3\",\n    \"Ba2Ca0.5Cu2Nd0.5O6.86Tl0.95\",\n    \"CBa4Ca0.7Cu5O14Y1.3\",\n    \"Ba2Ca0.15Cu3Er0.85O6.79\",\n    \"Bi0.5Ce0.85Cu2O8Sr1.5Y1.15\",\n    \"Ba1.43Ca0.11Cu3La1.33O7.06Pr0.03\",\n    \"Ba2Cu3NdO6.77\",\n    \"Ba1.8Ca0.19Cu3La0.2O7.08Y0.81\",\n    \"Al7.02Fe4.98Y\",\n    \"CuHg0.75Mo0.25O4.6Sr2\",\n    \"BaCaCu3LaO7.06\",\n    \"Ca0.25Cu2.24Er1.11O7Pb0.76Sr1.64\",\n    \"Ba1.6Cu3O6.35Sr0.4Yb\",\n    \"Ba2Ca0.15Co0.12Cu2.88O7.01Y0.85\",\n    \"Ba2CuHgO4.24\",\n    \"CuLa0.08O1.89Sr0.92\",\n    \"Ba2CuHg0.98O4.34\",\n    \"Ce0.27CuNd1.32O3.93Sr0.41\",\n    \"As6Ca3Fe7.038Pd1.962\",\n    \"CaCu2La2O6.037\",\n    \"Cu0.75Ni0.25O2Sr\",\n    \"Ba2CaCu2O8Tl1.81\",\n    \"BaCu2NdO7SrTl\",\n    \"CBaCuO5.05Sr\",\n    \"Ca0.1CuLa1.9O4.05\",\n    \"Ba1.7Ca0.2Cu3La0.3O7Y0.8\",\n    \"Cu2.2Ni0.8O8Pb2Sr2Y\",\n    \"Ba2Co0.084Cu2.916O7.01Y\",\n    \"Fe4GdP12\",\n    \"Ba2Cu2.7O6.75YZn0.3\",\n    \"Ba2Cu3NdO6.9\",\n    \"BaCa0.432Cu2O6.98Pr0.568SrTl0.964\",\n    \"Ba1.9Ca1.9Cu3O9Tl1.1\",\n    \"CuLa1.894O4Sr0.106\",\n    \"Ba1.91Cu3Na0.09O7Y\",\n    \"Ba1.25Ca0.48Cu3La0.98O6.94Pr0.23\",\n    \"Ba1.24Ca0.42Cu3La1.04O7.1Pr0.18\",\n    \"Ba1.85Cu3La0.15O7Y\",\n    \"As3CaCr0.84Fe3.16\",\n    \"CuEu0.21O1.94Sr0.79\",\n    \"C2Bi2Cu3O16Sr5\",\n    \"Fe0.2Ti0.8\",\n    \"Ce0.7Cu2Gd1.3O9.9RuSr2\",\n    \"Ca0.5Cu2Hg0.4O7Pr1.1Sr2\",\n    \"Cu2NdO7Sr2Tl\",\n    \"Ba2Ca1.95Cu3.25O9.952Tl1.66Y0.05\",\n    \"Ce0.5Cu2.5Mn0.5O9.15Sr2Y1.5\",\n    \"Ba2Cu2.78O7Y\",\n    \"Ba2Cu1.06O6Tl1.94\",\n    \"Bi2Ca1.7Cu3O10Pb0.3Sr2\",\n    \"Ba1.6Cu3O6.817Sr0.4Yb\",\n    \"Ba1.5Ca0.5Cu3LaO6.54\",\n    \"Ba2Ca0.8Cu2Nd0.2O6.86Tl0.96\",\n    \"Fe0.5V1.5Zr\",\n    \"Ba2Ca0.27Co0.3Cu2.7O7Y0.73\",\n    \"CuLa1.83Na0.16O4\",\n    \"Bi2Ca1.7Cu3F4O8Pb0.3Sr2\",\n    \"Ba2Cu2.8Ni0.2O6.85Y\",\n    \"Ba2Cu2.93O6.91Y\",\n    \"Fe0.4Mo6S8Sn\",\n    \"Ba2Cu2.94Li0.06O6.91Y\",\n    \"Ba2CuHg0.88O4.87S0.18\",\n    \"Ca0.65Cu2La1.6O6Pr0.35Sr0.4\",\n    \"Ba2Cu2.943Fe0.019O6.132Pr0.004Y0.997\",\n    \"Cu1.47Mo3S4\",\n    \"Ba1.28Ca0.4Cu3La1.11O7.14Y0.2\",\n    \"Bi2.1Ca1.12Cu2O8Sr1.78\",\n    \"Ba2Cu2.82Li0.18O6.77Y0.98\",\n    \"Ba2Co0.38Cu2.62O6.91Y\",\n    \"Ca0.36Cu2.17Lu0.97O7Pb0.83Sr1.67\",\n    \"Ba2Cu2.74Fe0.22O7.24Sm\",\n    \"Ba2CuHg0.752Mo0.252O4.584\",\n    \"Ce0.2CuNd1.8O4\",\n    \"Ba2Cu2.764Fe0.236O7.36Sm\",\n    \"Ba2Cu3O6.26Sm\",\n    \"Ba2Cu2.98O6.92Y\",\n    \"Ba1.5Cu3La0.5O7.22Y0.92\",\n    \"La₂PrNi₂O₇\",\n    \"Pr₄Ni₃O₁₀\",\n    \"(InSe₂)₀.₁₂NbSe₂\",\n    \"Sc\",\n    \"LaH₁₀\",\n    \"YH₉\",\n    \"YH₁₀\",\n    \"ThH₁₀\",\n    \"CaH₆\",\n    \"HgBa₂Ca₂Cu₃O₈\",\n    \"Tl₂Ba₂Ca₂Cu₃O₁₀\",\n    \"Bi₂Sr₂Ca₂Cu₃O₁₀\",\n    \"Tl₂Ba₂CaCu₂O₈\",\n    \"Bi₂Sr₂CaCu₂O₈\",\n    \"YBa₂Cu₃O₇\",\n    \"HgBa₂CaCu₂O₆\",\n    \"HgBa₂CuO₄\",\n    \"Tl₂Ba₂CuO₆\",\n    \"MgB₂\",\n    \"BaFe₂As₂\",\n    \"LaFeAsO\",\n    \"SmFeAsO\",\n    \"FeSe\",\n    \"LiFeAs\",\n    \"NaFeAs\",\n    \"Nb₃Sn\",\n    \"NbTi\",\n    \"V₃Si\",\n    \"CeCu₂Si₂\",\n    \"UPt₃\",\n    \"UBe₁₃\",\n    \"URu₂Si₂\",\n    \"CeCoIn₅\",\n    \"CeRhIn₅\",\n    \"K₃C₆₀\",\n    \"Cs₃C₆₀\",\n    \"NbSe₂\",\n    \"MoS₂\",\n    \"WS₂\",\n    \"Fe₃Sn₂\",\n    \"KV₃Sb₅\",\n    \"RbV₃Sb₅\",\n\"CsV₃Sb₅\",\n\"ScV₆Sn₆\",\n\"KV₃Bi₅\",\n\"CsV₃Bi₅\",\n\"Co₃Sn₂S₂\",\n\"Mn₃Sn\",\n\"Mn₃Ge\",\n\"ZrSiS\",\n\"Cd₃As₂\",\n\"Na₃Bi\",\n\"FeGe\",\n\n\"SrAuH3\",\n\"SrZnH3\",\n\"YSc2H24\",\n\"CaBC\",\n\"MgIrH\",\n\"Ti0.2Nb0.2Ta0.2Mo0.2W0.2C0.7N0.3\",\n\"La3Ni2O7\",\n\"(TaNb)0.67(HfZrTi)0.33\",\n\n#new ones\n\"CsTi3Bi5\",\n\"RbTi3Bi5\",\n\"KTi3Bi5\",\n\"Ti6Bi6\",\n\"CsV3Bi5\",\n\"KV3Bi5\",\n\"RbV3Bi5\",\n\"ScV6Sn6\",\n\"TbV6Sn6\",\n\"Na3Bi\",\n\"ZrSiSe\",\n\"ZrGeTe\",\n\"HfTe5\",\n\"TaIrTe4\",\n\"EuCd2As2\",\n\"MnBi2Te4\",\n\"CrAs\",\n\"UTe2\"\n]\n\nprint(f\"Total compositions to predict: {len(new_compositions)}\")\nprint(f\"Using {len(all_models)} models in ensemble\\n\")\n\nresults = []\nfailed_predictions = []\n\nfor idx, comp in enumerate(new_compositions, 1):\n    try:\n        mean_tc, std_tc = predict_tc_ensemble(\n            comp, all_models, all_scalers, element_features_dict, debug=False\n        )\n        \n        print(f\"{idx}/{len(new_compositions)}: {comp:<40} -> Tc = {mean_tc:7.2f} ± {std_tc:5.2f} K\")\n        \n        results.append({\n            'Composition': comp,\n            'Predicted_Tc_Mean': mean_tc,\n            'Predicted_Tc_Std': std_tc,\n            'Predicted_Tc_Min': mean_tc - std_tc,\n            'Predicted_Tc_Max': mean_tc + std_tc,\n            'CI_95_Lower': mean_tc - 1.96 * std_tc,\n            'CI_95_Upper': mean_tc + 1.96 * std_tc\n        })\n        \n    except Exception as e:\n        print(f\"{idx}/{len(new_compositions)}: {comp:<40} -> ERROR: {str(e)}\")\n        failed_predictions.append({'Composition': comp, 'Error': str(e)})\n        results.append({\n            'Composition': comp,\n            'Predicted_Tc_Mean': None,\n            'Predicted_Tc_Std': None,\n            'Predicted_Tc_Min': None,\n            'Predicted_Tc_Max': None,\n            'CI_95_Lower': None,\n            'CI_95_Upper': None\n        })\n\n\nprint(\"SAVING RESULTS\")\n\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"ensemble_predictions.csv\", index=False)\nprint(f\" Ensemble predictions saved to: ensemble_predictions.csv\")\nprint(f\"  Total predictions: {len(results)}\")\nprint(f\"  Successful: {len(results) - len(failed_predictions)}\")\nprint(f\"  Failed: {len(failed_predictions)}\")\n\n\n\nif failed_predictions:\n    df_failed = pd.DataFrame(failed_predictions)\n    df_failed.to_csv(\"failed_predictions.csv\", index=False)\n    print(f\"Failed predictions saved to: failed_predictions.csv\")\n\n\n\nprint(\"PREDICTION SUMMARY STATISTICS\")\n\ndf_successful = df_results[df_results['Predicted_Tc_Mean'].notna()]\n\nif len(df_successful) > 0:\n    print(f\"Predicted Tc Statistics:\")\n    print(f\"  Mean:   {df_successful['Predicted_Tc_Mean'].mean():.2f} K\")\n    print(f\"  Median: {df_successful['Predicted_Tc_Mean'].median():.2f} K\")\n    print(f\"  Std:    {df_successful['Predicted_Tc_Mean'].std():.2f} K\")\n    print(f\"  Min:    {df_successful['Predicted_Tc_Mean'].min():.2f} K\")\n    print(f\"  Max:    {df_successful['Predicted_Tc_Mean'].max():.2f} K\")\n    \n    print(f\"\\nAverage Prediction Uncertainty:\")\n    print(f\"  Mean Std: {df_successful['Predicted_Tc_Std'].mean():.2f} K\")\n    print(f\"  Median Std: {df_successful['Predicted_Tc_Std'].median():.2f} K\")\n    \n  \n    print(\"TOP 10 PREDICTED SUPERCONDUCTORS (by Tc)\")\n\n    top_10 = df_successful.nlargest(10, 'Predicted_Tc_Mean')\n    for i, row in enumerate(top_10.itertuples(), 1):\n        print(f\"{i:2d}. {row.Composition:<40} {row.Predicted_Tc_Mean:7.2f} ± {row.Predicted_Tc_Std:5.2f} K\")\n    \n    # Materials with low uncertainty\n    print(\"TOP 10 MOST CONFIDENT PREDICTIONS (lowest uncertainty)\")\n\n    low_uncertainty = df_successful.nsmallest(10, 'Predicted_Tc_Std')\n    for i, row in enumerate(low_uncertainty.itertuples(), 1):\n        print(f\"{i:2d}. {row.Composition:<40} {row.Predicted_Tc_Mean:7.2f} ± {row.Predicted_Tc_Std:5.2f} K\")\n    \n    # Plot distribution\n    print(\"GENERATING VISUALIZATION\")\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Plot 1: Histogram of predicted Tc\n    axes[0, 0].hist(df_successful['Predicted_Tc_Mean'], bins=50, \n                    color='steelblue', alpha=0.7, edgecolor='black')\n    axes[0, 0].set_xlabel('Predicted Tc (K)', fontsize=12)\n    axes[0, 0].set_ylabel('Count', fontsize=12)\n    axes[0, 0].set_title('Distribution of Predicted Tc', fontsize=14, fontweight='bold')\n    axes[0, 0].grid(alpha=0.3)\n    \n    # Plot 2: Histogram of uncertainties\n    axes[0, 1].hist(df_successful['Predicted_Tc_Std'], bins=50, \n                    color='coral', alpha=0.7, edgecolor='black')\n    axes[0, 1].set_xlabel('Prediction Uncertainty (Std, K)', fontsize=12)\n    axes[0, 1].set_ylabel('Count', fontsize=12)\n    axes[0, 1].set_title('Distribution of Prediction Uncertainties', fontsize=14, fontweight='bold')\n    axes[0, 1].grid(alpha=0.3)\n    \n    # Plot 3: Scatter plot - Tc vs Uncertainty\n    axes[1, 0].scatter(df_successful['Predicted_Tc_Mean'], \n                       df_successful['Predicted_Tc_Std'],\n                       alpha=0.5, s=30, c='green', edgecolors='black', linewidth=0.5)\n    axes[1, 0].set_xlabel('Predicted Tc (K)', fontsize=12)\n    axes[1, 0].set_ylabel('Prediction Uncertainty (K)', fontsize=12)\n    axes[1, 0].set_title('Predicted Tc vs Uncertainty', fontsize=14, fontweight='bold')\n    axes[1, 0].grid(alpha=0.3)\n    \n    # Plot 4: Top 20 compounds\n    top_20 = df_successful.nlargest(20, 'Predicted_Tc_Mean')\n    y_pos = np.arange(len(top_20))\n    axes[1, 1].barh(y_pos, top_20['Predicted_Tc_Mean'], \n                    xerr=top_20['Predicted_Tc_Std'],\n                    alpha=0.7, color='purple', capsize=3)\n    axes[1, 1].set_yticks(y_pos)\n    axes[1, 1].set_yticklabels(top_20['Composition'], fontsize=8)\n    axes[1, 1].set_xlabel('Predicted Tc (K)', fontsize=12)\n    axes[1, 1].set_title('Top 20 Predicted Superconductors', fontsize=14, fontweight='bold')\n    axes[1, 1].grid(alpha=0.3, axis='x')\n    axes[1, 1].invert_yaxis()\n    \n    plt.tight_layout()\n    plt.savefig('ensemble_predictions_summary.png', dpi=300, bbox_inches='tight')\n    print(\"Visualization saved to: ensemble_predictions_summary.png\")\n    plt.show()\n\n\nprint(\"ENSEMBLE PREDICTION COMPLETE!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"SHAP ANALYSIS - TOP 5 MODELS WITH NUMERICAL STABILITY\")\n\nimport shap\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Feature names (make sure this is defined)\nfeature_names = [\n    'atomic_number', 'atomic_volume', 'block_encoded', 'density', 'dipole_polarizability',\n    'electron_affinity', 'evaporation_heat', 'fusion_heat', 'group_id', 'lattice_constant',\n    'lattice_structure', 'melting_point', 'period', 'specific_heat', 'thermal_conductivity',\n    'vdw_radius', 'covalent_radius_pyykko', 'en_pauling', 'atomic_weight', 'atomic_radius_rahm',\n    'first_ionization_energy', 'valence_electrons', 'stoichiometry'\n]\n\n# ========================================================================\n# STEP 1: SELECT TOP 5 MODELS BASED ON TEST R²\n# ========================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SELECTING TOP 5 MODELS\")\nprint(\"=\"*80)\n\n# Load metrics to find best models\nmetrics_df = pd.read_csv('saved_models/all_runs_metrics.csv')\nprint(f\"\\nAll 50 models R² range: {metrics_df['test_r2'].min():.4f} to {metrics_df['test_r2'].max():.4f}\")\nprint(f\"Mean R²: {metrics_df['test_r2'].mean():.4f} ± {metrics_df['test_r2'].std():.4f}\")\n\n# Get indices of top 5 models\ntop_5_indices = metrics_df.nlargest(5, 'test_r2')['run'].values - 1  # -1 because run numbers start at 1\nprint(f\"\\nTop 5 model indices: {top_5_indices}\")\nprint(\"\\nTop 5 models performance:\")\nfor idx in top_5_indices:\n    r2 = metrics_df.iloc[idx]['test_r2']\n    mae = metrics_df.iloc[idx]['test_mae']\n    rmse = metrics_df.iloc[idx]['test_rmse']\n    print(f\"  Model {idx+1}: R²={r2:.4f}, MAE={mae:.2f}K, RMSE={rmse:.2f}K\")\n\n# Select top 5 models and scalers\ntop_5_models = [all_models[idx] for idx in top_5_indices]\ntop_5_scalers = [all_scalers[idx] for idx in top_5_indices]\n\n# ========================================================================\n# STEP 2: PREPARE DATA FOR SHAP ANALYSIS\n# ========================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"PREPARING DATA FOR SHAP ANALYSIS\")\nprint(\"=\"*80)\n\n# Use fewer samples for numerical stability\nX_common_test = X_deepset[:1000]\ny_common_test = y[:1000]\n\nnum_background_samples = 50   # Reduced for stability\nnum_explain_samples = 100      # Reduced for stability\n\nprint(f\"Using {len(X_common_test)} total samples\")\nprint(f\"Background samples: {num_background_samples}\")\nprint(f\"Samples to explain: {num_explain_samples}\")\n\n# ========================================================================\n# STEP 3: COMPUTE SHAP VALUES WITH NUMERICAL STABILITY\n# ========================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPUTING SHAP VALUES (TOP 5 MODELS)\")\nprint(\"=\"*80)\n\nall_shap_values = []\n\nfor model_num, (model_idx, model, scaler) in enumerate(zip(top_5_indices, top_5_models, top_5_scalers)):\n    print(f\"\\nModel {model_num + 1}/5 (Run {model_idx + 1}): Computing SHAP values...\")\n    \n    # Normalize the test data with this model's scaler\n    X_test_normalized = scaler.transform(X_common_test.reshape(-1, 23)).reshape(-1, max_elements, 23)\n    \n    # Create background dataset\n    background_data = X_test_normalized[:num_background_samples]\n    \n    # Data to explain\n    data_to_explain = X_test_normalized[num_background_samples:num_background_samples+num_explain_samples]\n    \n    # ========================================================================\n    # KEY FIX: Use element-wise SHAP instead of flattened\n    # ========================================================================\n    # Instead of flattening to 230D, we'll compute SHAP per element position\n    # and aggregate, which is more numerically stable\n    \n    def model_predict(x):\n        \"\"\"Wrapper function for SHAP that handles reshaping\"\"\"\n        if len(x.shape) == 2:\n            # x is (batch_size, 230) - reshape to (batch_size, 10, 23)\n            x = x.reshape(-1, max_elements, 23)\n        return model.predict(x, verbose=0)\n    \n    # Flatten for KernelExplainer\n    background_flat = background_data.reshape(num_background_samples, -1)\n    explain_flat = data_to_explain.reshape(num_explain_samples, -1)\n    \n    try:\n        # Use fewer samples for KernelExplainer to improve stability\n        explainer = shap.KernelExplainer(model_predict, background_flat)\n        \n        np.random.seed(42 + model_num)  # Different seed per model\n        \n        # Compute SHAP values with reduced samples\n        shap_values = explainer.shap_values(explain_flat, nsamples=50)  # Further reduced\n        \n        # Handle list output\n        if isinstance(shap_values, list):\n            shap_values = shap_values[0]\n        \n        # ========================================================================\n        # CRITICAL: CLIP EXTREME VALUES TO PREVENT NUMERICAL EXPLOSIONS\n        # ========================================================================\n        # Sometimes SHAP can produce extremely large values due to numerical issues\n        # We'll clip at 99.9th percentile to remove outliers\n        shap_abs = np.abs(shap_values)\n        threshold = np.percentile(shap_abs, 99.9)\n        shap_values_clipped = np.clip(shap_values, -threshold, threshold)\n        \n        print(f\"  Raw SHAP range: [{shap_values.min():.2e}, {shap_values.max():.2e}]\")\n        print(f\"  Clipped at: ±{threshold:.2e}\")\n        print(f\"  After clipping: [{shap_values_clipped.min():.2e}, {shap_values_clipped.max():.2e}]\")\n        \n        # Reshape back to (num_samples, max_elements, 23)\n        shap_values_reshaped = shap_values_clipped.reshape(num_explain_samples, max_elements, 23)\n        all_shap_values.append(shap_values_reshaped)\n        \n        print(f\"  ✓ Completed model {model_num + 1}\")\n        \n    except Exception as e:\n        print(f\"  ✗ Error with model {model_num + 1}: {e}\")\n        import traceback\n        traceback.print_exc()\n        continue\n\nprint(f\"\\n✓ SHAP values computed for {len(all_shap_values)} models!\")\n\nif len(all_shap_values) == 0:\n    print(\"\\n❌ ERROR: No SHAP values were successfully computed!\")\n    print(\"This suggests a fundamental issue with the model or data.\")\n    exit(1)\n\n# ========================================================================\n# STEP 4: AGGREGATE SHAP VALUES USING ABSOLUTE VALUES\n# ========================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"AGGREGATING SHAP VALUES\")\nprint(\"=\"*80)\n\n# Convert to numpy array\nall_shap_values = np.array(all_shap_values)  # Shape: (n_models, num_samples, max_elements, 23)\nprint(f\"All SHAP values shape: {all_shap_values.shape}\")\n\n# Take absolute value BEFORE averaging\nall_shap_abs = np.abs(all_shap_values)\n\n# Average across models first, then across samples and elements\nensemble_shap_abs = np.mean(all_shap_abs, axis=0)  # Shape: (num_samples, max_elements, 23)\nprint(f\"Ensemble absolute SHAP shape: {ensemble_shap_abs.shape}\")\n\n# Flatten across samples and elements to get feature importance\nensemble_shap_abs_2d = ensemble_shap_abs.reshape(-1, 23)\n\n# Calculate global feature importance\nmean_abs_shap = np.mean(ensemble_shap_abs_2d, axis=0)\nmedian_abs_shap = np.median(ensemble_shap_abs_2d, axis=0)  # Median is more robust to outliers\nshap_importance_std = np.std(ensemble_shap_abs_2d, axis=0)\n\nprint(f\"Feature importance shape: {mean_abs_shap.shape}\")\nprint(f\"Mean feature importance range: [{mean_abs_shap.min():.6f}, {mean_abs_shap.max():.6f}]\")\nprint(f\"Median feature importance range: [{median_abs_shap.min():.6f}, {median_abs_shap.max():.6f}]\")\n\n# ========================================================================\n# STEP 5: DISPLAY AND SAVE RESULTS\n# ========================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"GLOBAL FEATURE IMPORTANCE (Top 5 Models)\")\nprint(\"=\"*80)\nprint(f\"\\n{'Rank':<6} {'Feature':<30} {'Mean |SHAP|':<14} {'Median |SHAP|':<14} {'Std Dev':<12}\")\nprint(\"-\" * 80)\n\n# Sort by MEDIAN (more robust to outliers)\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Mean_Abs_SHAP': mean_abs_shap,\n    'Median_Abs_SHAP': median_abs_shap,\n    'Std_Dev': shap_importance_std,\n})\nimportance_df['Rank'] = importance_df['Median_Abs_SHAP'].rank(ascending=False)\nimportance_df = importance_df.sort_values('Median_Abs_SHAP', ascending=False)\n\nfor idx, row in importance_df.iterrows():\n    print(f\"{int(row['Rank']):<6} {row['Feature']:<30} {row['Mean_Abs_SHAP']:<14.6f} \"\n          f\"{row['Median_Abs_SHAP']:<14.6f} {row['Std_Dev']:<12.6f}\")\n\n# Save results\nimportance_df.to_csv('saved_models/shap_importance_top5_models.csv', index=False)\nnp.save('saved_models/shap_values_top5_models.npy', ensemble_shap_abs)\n\nprint(\"\\n✓ Results saved to:\")\nprint(\"  - saved_models/shap_importance_top5_models.csv\")\nprint(\"  - saved_models/shap_values_top5_models.npy\")\n\n# ========================================================================\n# STEP 6: VISUALIZATIONS\n# ========================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATING VISUALIZATIONS\")\nprint(\"=\"*80)\n\n# 1. Bar plot with median importance (more robust)\nprint(\"\\n1. Creating feature importance bar plot...\")\nfig, ax = plt.subplots(figsize=(12, 8))\n\nsorted_importance = importance_df.sort_values('Median_Abs_SHAP', ascending=True)\ny_pos = np.arange(len(sorted_importance))\n\nax.barh(y_pos, sorted_importance['Median_Abs_SHAP'], \n        alpha=0.8, color='steelblue', label='Median |SHAP|')\nax.set_yticks(y_pos)\nax.set_yticklabels(sorted_importance['Feature'], fontsize=10)\nax.set_xlabel('Median |SHAP Value| (Top 5 Models)', fontsize=12)\nax.set_title('Feature Importance Based on SHAP\\n(Using Top 5 Best Performing Models)', \n             fontsize=14, fontweight='bold')\nax.grid(axis='x', alpha=0.3)\nax.legend()\n\nplt.tight_layout()\nplt.savefig('saved_models/shap_importance_top5_bars.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# 2. Top 10 features\nprint(\"\\n2. Creating top 10 features plot...\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\ntop_10 = importance_df.head(10).sort_values('Median_Abs_SHAP', ascending=True)\ny_pos = np.arange(len(top_10))\n\ncolors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_10)))\nbars = ax.barh(y_pos, top_10['Median_Abs_SHAP'], color=colors, alpha=0.8)\n\n# Add value labels\nfor i, (bar, v) in enumerate(zip(bars, top_10['Median_Abs_SHAP'])):\n    ax.text(v, i, f' {v:.4f}', va='center', fontsize=9)\n\nax.set_yticks(y_pos)\nax.set_yticklabels(top_10['Feature'], fontsize=11, fontweight='bold')\nax.set_xlabel('Median |SHAP Value|', fontsize=12)\nax.set_title('Top 10 Most Important Features\\n(Top 5 Models)', \n             fontsize=14, fontweight='bold')\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('saved_models/shap_top10_features.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# 3. Comparison: Mean vs Median importance\nprint(\"\\n3. Creating mean vs median comparison...\")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n\n# Sort by median for consistent ordering\nsorted_df = importance_df.sort_values('Median_Abs_SHAP', ascending=True)\ny_pos = np.arange(len(sorted_df))\n\n# Mean importance\nax1.barh(y_pos, sorted_df['Mean_Abs_SHAP'], alpha=0.7, color='coral')\nax1.set_yticks(y_pos)\nax1.set_yticklabels(sorted_df['Feature'], fontsize=9)\nax1.set_xlabel('Mean |SHAP Value|', fontsize=12)\nax1.set_title('Feature Importance - Mean', fontsize=13, fontweight='bold')\nax1.grid(axis='x', alpha=0.3)\n\n# Median importance  \nax2.barh(y_pos, sorted_df['Median_Abs_SHAP'], alpha=0.7, color='steelblue')\nax2.set_yticks(y_pos)\nax2.set_yticklabels(sorted_df['Feature'], fontsize=9)\nax2.set_xlabel('Median |SHAP Value|', fontsize=12)\nax2.set_title('Feature Importance - Median (More Robust)', fontsize=13, fontweight='bold')\nax2.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('saved_models/shap_mean_vs_median.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# 4. Distribution of SHAP values per feature (boxplot)\nprint(\"\\n4. Creating SHAP distribution boxplot...\")\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Prepare data for boxplot\nshap_data_by_feature = [ensemble_shap_abs_2d[:, i] for i in range(23)]\n\nbp = ax.boxplot(shap_data_by_feature, positions=range(23), widths=0.6,\n                patch_artist=True, showfliers=False)\n\nfor patch in bp['boxes']:\n    patch.set_facecolor('lightblue')\n    patch.set_alpha(0.7)\n\nax.set_xticks(range(23))\nax.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=9)\nax.set_ylabel('|SHAP Value| Distribution', fontsize=12)\nax.set_title('Distribution of Absolute SHAP Values Per Feature\\n(Across all samples)', \n             fontsize=14, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('saved_models/shap_distribution_boxplot.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SHAP ANALYSIS COMPLETE!\")\nprint(\"=\"*80)\nprint(\"\\nKey Findings:\")\nprint(f\"✓ Most Important Feature:  {importance_df.iloc[0]['Feature']} \"\n      f\"(Median SHAP = {importance_df.iloc[0]['Median_Abs_SHAP']:.6f})\")\nprint(f\"✓ 2nd Most Important:      {importance_df.iloc[1]['Feature']} \"\n      f\"(Median SHAP = {importance_df.iloc[1]['Median_Abs_SHAP']:.6f})\")\nprint(f\"✓ 3rd Most Important:      {importance_df.iloc[2]['Feature']} \"\n      f\"(Median SHAP = {importance_df.iloc[2]['Median_Abs_SHAP']:.6f})\")\nprint(f\"\\n✓ Least Important Feature: {importance_df.iloc[-1]['Feature']} \"\n      f\"(Median SHAP = {importance_df.iloc[-1]['Median_Abs_SHAP']:.6f})\")\n\nprint(f\"\\n✓ Analysis based on top 5 best models (out of 50)\")\nprint(f\"✓ Used median aggregation for robustness to outliers\")\nprint(f\"✓ Applied numerical clipping to prevent SHAP explosions\")\nprint(f\"✓ All plots saved to saved_models/ directory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T04:58:55.382051Z","iopub.execute_input":"2025-11-08T04:58:55.382229Z","iopub.status.idle":"2025-11-08T04:59:10.540423Z","shell.execute_reply.started":"2025-11-08T04:58:55.382214Z","shell.execute_reply":"2025-11-08T04:59:10.539353Z"}},"outputs":[],"execution_count":null}]}